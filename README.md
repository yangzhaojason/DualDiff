# DualDiff
A dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences.

<!-- ## Demo Video
<video width="800" controls>
  <source src="https://github.com/yangzhaojason/DualDiff/raw/refs/heads/main/media/5721_1739810373.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video> -->

## Videos generated by DualDiff+
<img src="media/clip_1.gif" width="1000" alt="generated_video">
<img src="media/clip_2.gif" width="1000" alt="generated_video">
<img src="media/clip_3.gif" width="1000" alt="generated_video">
<img src="media/clip_4.gif" width="1000" alt="generated_video">

## Abstract
Abstractâ€”Accurate and high-fidelity driving scene reconstruction demands the effective utilization of comprehensive scene information as conditional inputs. Existing methods predominantly rely on 3D bounding boxes and BEV road maps for
foreground and background control, which fail to capture the full complexity of driving scenes and adequately integrate multimodal information. In this work, we present DualDiff, a dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences. Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional input, offering rich foreground and background semantics alongside 3D spatial geometry to precisely control the generation of both elements. To improve the synthesis of finegrained foreground objects, particularly complex and distant ones, we propose a Foreground-Aware Mask (FGM) denoising loss function. Additionally, we develop the Semantic Fusion Attention (SFA) mechanism to dynamically prioritize relevant information and suppress noise, enabling more effective multimodal fusion. Finally, to ensure high-quality image-to-video generation, we introduce the Reward-Guided Diffusion (RGD) framework, which maintains global consistency and semantic coherence in generated videos. Extensive experiments demonstrate that DualDiff achieves state-of-the-art (SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff reduces the FID score by 4.09% compared to the best baseline. In downstream tasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and road mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP increases by 1.46%.

## ðŸ“¢ News
- **[2025-01-28]** âœ¨ DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion accepted by ICRA2025.
- **[2025-03-07]** ðŸš€ DualDiff+: Dual-Branch Diffusion for High-Fidelity
Video Generation with Reward Guidance is under review.

## Method
Architecture Overview of DualDiff for Video Generation. The model uses Occupancy Ray-shape Sampling (ORS) and Semantic Fusion Attention (SFA) for scene representation, which are fed into a dual-branch foreground-background architecture. The outputs are merged through residual connections in a U-Net. Video generation follows a Two-stage optimization: Spatio-Temporal Attention (ST-Attn) and Temporal Attention (Temporal Attn) are trained in the first stage, while Reward-Guided Diffusion (RGD) and Low-Rank Adaptation (LoRA) fine-tune the attention in the second stage.
<!-- <img src="https://github.com/yangzhaojason/DualDiff/blob/main/media/framework.jpg" width="800"> -->
<img src="https://yangzhaojason.github.io/DualDiff/media/framework.jpg" width="800">

## Getting Started
### Environment Setup
Clone this repo with submodules

```bash
git clone --recursive https://github.com/yangzhaojason/DualDiff.git
```

The code is tested with `Pytorch==1.10.2` and `cuda 11.3` on A800 servers. To setup the python environment, follow:

```bash
conda create -n dualdiff python=3.8
conda activate dualdiff
pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
cd ${ROOT}
pip install -r requirements/dev.txt
cd third_party/xformers
pip install -e .
cd ../diffusers
pip install -e .
cd ../bevfusion
python setup.py develop
cd ../../

```

### Prepare Data
1. Download the nuScenes dataset from the [website](https://www.nuscenes.org/nuscenes) and put them in `./data/`. You should have these files:
    ```bash
    data/nuscenes
    â”œâ”€â”€ maps
    â”œâ”€â”€ mini
    â”œâ”€â”€ samples
    â”œâ”€â”€ sweeps
    â”œâ”€â”€ v1.0-mini
    â””â”€â”€ v1.0-trainval
    ```

> [!TIP]
> You can download the `.pkl` files from [OneDrive](https://mycuhk-my.sharepoint.com/:u:/g/personal/1155157018_link_cuhk_edu_hk/EYF9ZkMHwVZKjrU5CUUPbfYBhC1iZMMnhE2uI2q5iCuv9w?e=QgEmcH). They should be enough for training and testing.

2. Generate mmdet3d annotation files by:

    ```bash
    python tools/create_data.py nuscenes --root-path ./data/nuscenes \
      --out-dir ./data/nuscenes_mmdet3d_2 --extra-tag nuscenes
    ```
    You should have these files:
    ```bash
    data/nuscenes_mmdet3d_2
    â”œâ”€â”€ nuscenes_dbinfos_train.pkl (-> ${bevfusion-version}/nuscenes_dbinfos_train.pkl)
    â”œâ”€â”€ nuscenes_gt_database (-> ${bevfusion-version}/nuscenes_gt_database)
    â”œâ”€â”€ nuscenes_infos_train.pkl
    â””â”€â”€ nuscenes_infos_val.pkl
    ```
    Note: As shown above, some files can be soft-linked with the original version from bevfusion. If some of the files is located in `data/nuscenes`, you can move them to `data/nuscenes_mmdet3d_2` manually.
3. Download map annotation pkl from one drive [nuscenes_map_anns_train.pkl](https://drive.google.com/file/d/1lrlHt4Amry4A_jxfzrm3U1exqpCDQ936/view?usp=sharing) and [nuscenes_map_anns_val.pkl](https://drive.google.com/file/d/15oQs8I9ejQMVERsqqk89w-_H_oixGjqp/view?usp=sharing). You should have these files:
    ```bash
    root
    â”œâ”€â”€ data
    â”œâ”€â”€ magicdrive
    â”œâ”€â”€ third_party
    â”œâ”€â”€ nuscenes_map_anns_val.pkl
    â””â”€â”€ nuscenes_map_anns_train.pkl
    â”œâ”€â”€ ...
    ```
4. Download occ_proj from ...
### Pretrained Weights

### Train DualDiff

### Evaluation

#### To test FID&FVD
#### To test Downstream

## Quantitative Results
More results can be found in the main paper.
<!-- <img src="https://github.com/yangzhaojason/DualDiff/blob/main/media/vis_result.jpg" width="700"> -->
<img src="https://yangzhaojason.github.io/DualDiff/media/vis_result.jpg" width="700">

## Citation
If you find our work useful, please cite our paper:

```bibtex
@article{yang2025dualdiff+,
  title={DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance},
  author={Yang, Zhao and Qian, Zezhong and Li, Xiaofan and Xu, Weixiang and Zhao, Gongpeng and Yu, Ruohong and Zhu, Lingsi and Liu, Longjun},
  journal={arXiv preprint arXiv:2503.03689},
  year={2025}
}
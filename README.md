# DualDiff
A dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences.

<!-- ## Demo Video
<video width="800" controls>
  <source src="https://github.com/yangzhaojason/DualDiff/raw/refs/heads/main/media/5721_1739810373.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video> -->

## Videos generated by DualDiff
<video width="800" controls>
  <source src="https://cdn.jsdelivr.net/gh/yangzhaojason/DualDiff/media/5721_1739810373.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Abstract
Abstract‚ÄîAccurate and high-fidelity driving scene reconstruction demands the effective utilization of comprehensive scene information as conditional inputs. Existing methods predominantly rely on 3D bounding boxes and BEV road maps for
foreground and background control, which fail to capture the full complexity of driving scenes and adequately integrate multimodal information. In this work, we present DualDiff, a dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences. Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional input, offering rich foreground and background semantics alongside 3D spatial geometry to precisely control the generation of both elements. To improve the synthesis of finegrained foreground objects, particularly complex and distant ones, we propose a Foreground-Aware Mask (FGM) denoising loss function. Additionally, we develop the Semantic Fusion Attention (SFA) mechanism to dynamically prioritize relevant information and suppress noise, enabling more effective multimodal fusion. Finally, to ensure high-quality image-to-video generation, we introduce the Reward-Guided Diffusion (RGD) framework, which maintains global consistency and semantic coherence in generated videos. Extensive experiments demonstrate that DualDiff achieves state-of-the-art (SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff reduces the FID score by 4.09% compared to the best baseline. In downstream tasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and road mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP increases by 1.46%.
## News
## üì¢ News
- **[2025-01-28]** ‚ú® DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion Ë¢´ICRA2025Êé•Âèó


## Method
Architecture Overview of DualDiff for Video Generation. The model uses Occupancy Ray-shape Sampling (ORS) and Semantic Fusion Attention (SFA) for scene representation, which are fed into a dual-branch foreground-background architecture. The outputs are merged through residual connections in a U-Net. Video generation follows a Two-stage optimization: Spatio-Temporal Attention (ST-Attn) and Temporal Attention (Temporal Attn) are trained in the first stage, while Reward-Guided Diffusion (RGD) and Low-Rank Adaptation (LoRA) fine-tune the attention in the second stage. \
<!-- <img src="https://github.com/yangzhaojason/DualDiff/blob/main/media/framework.jpg" width="800"> -->
<img src="https://yangzhaojason.github.io/DualDiff/media/framework.jpg" width="800">

## Getting Started

### Environment Setup

### Pretrained Weights

### Train DualDiff

### Evaluation

#### To test FID&FVD
#### To test Downstream

## Quantitative Results
More results can be found in the main paper. \
<!-- <img src="https://github.com/yangzhaojason/DualDiff/blob/main/media/vis_result.jpg" width="700"> -->
<img src="https://yangzhaojason.github.io/DualDiff/media/vis_result.jpg" width="700">

## Citation
If you find our work useful, please cite our paper:

```bibtex
@article{yang2025dualdiff+,
  title={DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance},
  author={Yang, Zhao and Qian, Zezhong and Li, Xiaofan and Xu, Weixiang and Zhao, Gongpeng and Yu, Ruohong and Zhu, Lingsi and Liu, Longjun},
  journal={arXiv preprint arXiv:2503.03689},
  year={2025}
}